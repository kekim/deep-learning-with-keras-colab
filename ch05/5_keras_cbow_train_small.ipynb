{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_word2vec_skipgram.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"HFF8f2fXSHVq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"5e38bcfa-5485-483a-c8cd-20a8a7d3e24b","executionInfo":{"status":"ok","timestamp":1541210273816,"user_tz":-540,"elapsed":2858,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["!mkdir data\n","!wget http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt -O ./data/alice_in_wonderland.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2018-11-03 01:57:53--  http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt\n","Resolving www.umich.edu (www.umich.edu)... 141.211.243.251, 2607:f018:1:1::1\n","Connecting to www.umich.edu (www.umich.edu)|141.211.243.251|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 148545 (145K) [text/plain]\n","Saving to: ‘./data/alice_in_wonderland.txt’\n","\n","\r          ./data/al   0%[                    ]       0  --.-KB/s               \r./data/alice_in_won 100%[===================>] 145.06K  --.-KB/s    in 0.1s    \n","\n","2018-11-03 01:57:53 (1.08 MB/s) - ‘./data/alice_in_wonderland.txt’ saved [148545/148545]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"hxkdU_GCSqZN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fbbc9a32-8986-421d-c287-6a0d2ea1321a","executionInfo":{"status":"ok","timestamp":1541210337989,"user_tz":-540,"elapsed":2522,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","from __future__ import print_function\n","import operator\n","\n","import nltk\n","import numpy as np\n","from keras.callbacks import TensorBoard\n","from keras.layers import Dense, Dropout, Activation\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer, one_hot\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics.pairwise import cosine_distances\n","from sklearn.preprocessing import OneHotEncoder\n","import codecs\n","\n","np.random.seed(42)\n","\n","LOG_DIR = './logs'\n","BATCH_SIZE = 128\n","NUM_EPOCHS = 20"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"G9KUIc3lSwsG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"d3ce59a3-3e63-4777-b155-59892f3317ba","executionInfo":{"status":"ok","timestamp":1541210653428,"user_tz":-540,"elapsed":597,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["with codecs.open(\"./data/alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n","    lines = [line.strip() for line in f if len(line) != 0]\n","\n","try:\n","    sents = nltk.sent_tokenize(\" \".join(lines))\n","except LookupError:\n","    print(\"Englisth tokenize does not downloaded. So download it.\")\n","    nltk.download(\"punkt\")\n","    sents = nltk.sent_tokenize(\" \".join(lines))\n","\n","\n","tokenizer = Tokenizer(5000)  # use top 5000 words only\n","tokens = tokenizer.fit_on_texts(sents)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","w_lefts, w_centers, w_rights = [], [], []\n","for sent in sents:\n","    embedding = one_hot(sent, vocab_size)\n","    triples = list(nltk.trigrams(embedding))\n","    w_lefts.extend([x[0] for x in triples])\n","    w_centers.extend([x[1] for x in triples])\n","    w_rights.extend([x[2] for x in triples])\n","\n","print(w_lefts[0], w_centers[0], w_rights[0])\n","print(w_lefts[1], w_centers[1], w_rights[1])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["2323 883 1709\n","883 1709 2476\n"],"name":"stdout"}]},{"metadata":{"id":"CQ7Zf7WATG4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"88d9d59e-da99-4e66-9670-8d021d03ea02","executionInfo":{"status":"ok","timestamp":1541210660664,"user_tz":-540,"elapsed":2808,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["ohe = OneHotEncoder(n_values=vocab_size)\n","Xleft = ohe.fit_transform(np.array(w_lefts).reshape(-1, 1)).todense()\n","Xright = ohe.fit_transform(np.array(w_rights).reshape(-1, 1)).todense()\n","X = (Xleft + Xright) / 2.0\n","Y = ohe.fit_transform(np.array(w_centers).reshape(-1, 1)).todense()\n","Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3,\n","                                                random_state=42)\n","print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(17201, 2653) (7372, 2653) (17201, 2653) (7372, 2653)\n"],"name":"stdout"}]},{"metadata":{"id":"YuvE60D2UGaa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"40a56c37-3217-47f3-8f8f-27167bd2ee25","executionInfo":{"status":"ok","timestamp":1541210680612,"user_tz":-540,"elapsed":642,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["model = Sequential()\n","model.add(Dense(300, input_shape=(Xtrain.shape[1],)))\n","model.add(Activation(\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(Ytrain.shape[1]))\n","model.add(Activation(\"softmax\"))\n","model.summary()\n","\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", \n","              metrics=[\"accuracy\"])\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 300)               796200    \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 300)               0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 300)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2653)              798553    \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 2653)              0         \n","=================================================================\n","Total params: 1,594,753\n","Trainable params: 1,594,753\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"BAiW-TK_ULlz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":755},"outputId":"466c400f-545d-4720-9bbb-f33620a68570","executionInfo":{"status":"ok","timestamp":1541210755778,"user_tz":-540,"elapsed":63483,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, \n","                    epochs=NUM_EPOCHS, verbose=1,\n","                    callbacks=[TensorBoard(LOG_DIR)],\n","                    validation_data=(Xtest, Ytest))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train on 17201 samples, validate on 7372 samples\n","Epoch 1/20\n","17201/17201 [==============================] - 3s 195us/step - loss: 6.5189 - acc: 0.0627 - val_loss: 5.8508 - val_acc: 0.0585\n","Epoch 2/20\n","17201/17201 [==============================] - 2s 143us/step - loss: 5.7321 - acc: 0.0642 - val_loss: 5.7765 - val_acc: 0.0585\n","Epoch 3/20\n","17201/17201 [==============================] - 3s 147us/step - loss: 5.6365 - acc: 0.0642 - val_loss: 5.7308 - val_acc: 0.0585\n","Epoch 4/20\n","17201/17201 [==============================] - 3s 159us/step - loss: 5.5506 - acc: 0.0683 - val_loss: 5.6707 - val_acc: 0.0676\n","Epoch 5/20\n","17201/17201 [==============================] - 3s 151us/step - loss: 5.4603 - acc: 0.0785 - val_loss: 5.6005 - val_acc: 0.0838\n","Epoch 6/20\n","17201/17201 [==============================] - 3s 149us/step - loss: 5.3654 - acc: 0.0946 - val_loss: 5.5428 - val_acc: 0.1026\n","Epoch 7/20\n","17201/17201 [==============================] - 2s 143us/step - loss: 5.2771 - acc: 0.1142 - val_loss: 5.4856 - val_acc: 0.1213\n","Epoch 8/20\n","17201/17201 [==============================] - 2s 145us/step - loss: 5.1904 - acc: 0.1297 - val_loss: 5.4318 - val_acc: 0.1291\n","Epoch 9/20\n","17201/17201 [==============================] - 3s 147us/step - loss: 5.1109 - acc: 0.1396 - val_loss: 5.3930 - val_acc: 0.1352\n","Epoch 10/20\n","17201/17201 [==============================] - 2s 143us/step - loss: 5.0415 - acc: 0.1492 - val_loss: 5.3526 - val_acc: 0.1408\n","Epoch 11/20\n","17201/17201 [==============================] - 2s 143us/step - loss: 4.9766 - acc: 0.1563 - val_loss: 5.3194 - val_acc: 0.1453\n","Epoch 12/20\n","17201/17201 [==============================] - 3s 147us/step - loss: 4.9118 - acc: 0.1648 - val_loss: 5.2942 - val_acc: 0.1499\n","Epoch 13/20\n","17201/17201 [==============================] - 3s 155us/step - loss: 4.8531 - acc: 0.1711 - val_loss: 5.2718 - val_acc: 0.1536\n","Epoch 14/20\n","17201/17201 [==============================] - 3s 157us/step - loss: 4.8015 - acc: 0.1762 - val_loss: 5.2572 - val_acc: 0.1555\n","Epoch 15/20\n","17201/17201 [==============================] - 3s 155us/step - loss: 4.7488 - acc: 0.1808 - val_loss: 5.2359 - val_acc: 0.1560\n","Epoch 16/20\n","17201/17201 [==============================] - 3s 154us/step - loss: 4.6984 - acc: 0.1850 - val_loss: 5.2293 - val_acc: 0.1590\n","Epoch 17/20\n","17201/17201 [==============================] - 3s 156us/step - loss: 4.6542 - acc: 0.1884 - val_loss: 5.2054 - val_acc: 0.1594\n","Epoch 18/20\n","17201/17201 [==============================] - 3s 155us/step - loss: 4.6008 - acc: 0.1953 - val_loss: 5.2058 - val_acc: 0.1625\n","Epoch 19/20\n","17201/17201 [==============================] - 3s 154us/step - loss: 4.5610 - acc: 0.2000 - val_loss: 5.1992 - val_acc: 0.1655\n","Epoch 20/20\n","17201/17201 [==============================] - 3s 157us/step - loss: 4.5152 - acc: 0.2048 - val_loss: 5.1923 - val_acc: 0.1674\n"],"name":"stdout"}]},{"metadata":{"id":"zSqYMji0UVDH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"644197f3-7d7d-46dc-dc89-a2d3601a4559","executionInfo":{"status":"ok","timestamp":1541210766017,"user_tz":-540,"elapsed":1567,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["# evaluate model\n","score = model.evaluate(Xtest, Ytest, verbose=1)\n","print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["7372/7372 [==============================] - 1s 136us/step\n","Test score: 5.192, accuracy: 0.167\n"],"name":"stdout"}]},{"metadata":{"id":"j-AcLFQmo7hi","colab_type":"code","outputId":"f25e608b-ba64-448e-88d5-ef9cfb5dfdcc","executionInfo":{"status":"ok","timestamp":1541210833027,"user_tz":-540,"elapsed":20441,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["# using the word2vec model\n","word2idx = tokenizer.word_index\n","idx2word = {v: k for k, v in word2idx.items()}\n","\n","# retrieve the weights from the first dense layer. This will convert\n","# the input vector from a one-hot sum of two words to a dense 300 \n","# dimensional representation\n","W, b = model.layers[0].get_weights()\n","\n","idx2emb = {}    \n","for word in word2idx.keys():\n","    wid = word2idx[word]\n","    vec_in = ohe.fit_transform(np.array(wid)).todense()\n","    vec_emb = np.dot(vec_in, W)\n","    idx2emb[wid] = vec_emb\n","\n","for word in [\"stupid\", \"alice\", \"succeeded\"]:\n","    wid = word2idx[word]\n","    source_emb = idx2emb[wid]\n","    distances = []\n","    for i in range(1, vocab_size):\n","        if i == wid:\n","            continue\n","        target_emb = idx2emb[i]\n","        distances.append(((wid, i), \n","                         cosine_distances(source_emb, target_emb)))\n","    sorted_distances = sorted(distances, key=operator.itemgetter(1))[0:10]\n","    predictions = [idx2word[x[0][1]] for x in sorted_distances]\n","    print(\"{:s} => {:s}\".format(word, \", \".join(predictions)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["stupid => tried, here, tale, mean, may, slowly, ashamed, quarrelled, wandering, between\n","alice => anger, pointing, stay, blame, seated, into, milk, contemptuously, shakespeare, figures\n","succeeded => memory, feeble, speech, spirited, paint, patience, able, what, rabbit', we've\n"],"name":"stdout"}]},{"metadata":{"id":"TXiex_YFUfwp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-7nCfeM1pKD0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}